{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsDgZnzslaZA"
      },
      "outputs": [],
      "source": [
        " -# Explain the role of activation functions in neural networks. Compare and contrast linear and nonlinear\n",
        "#activation functions. Why are nonlinear activation functions preferred in hidden layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation functions play a crucial role in neural networks by determining how the weighted sum of inputs is transformed into an output from a node or neuron. They introduce non-linearity into the model, enabling neural networks to learn complex patterns, relationships, and representations from the data. Without activation functions, neural networks would essentially behave as linear models, unable to model real-world non-linear data.\n",
        "\n",
        "Types of Activation Functions:\n",
        "Linear Activation Function:\n",
        "\n",
        "Definition: The linear activation function outputs a value that is proportional to the input. This can be expressed as\n",
        "𝑓\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝑥\n",
        "f(x)=x, where the output is directly equal to the input.\n",
        "Advantages: Simplicity, and ease of mathematical operations (like differentiation).\n",
        "Disadvantages: In a multi-layer neural network, multiple linear layers collapse into a single linear transformation. This means that no matter how many layers you stack, the output is just a linear transformation of the input, limiting the network’s ability to model complex functions.\n",
        "𝑓\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝑎\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        "(where\n",
        "𝑎\n",
        " and\n",
        "𝑏\n",
        " are constants)\n",
        "f(x)=ax+b(where a and b are constants)\n",
        "In such a scenario, backpropagation would not be effective, as the gradient (derivative) would be constant. This leads to no learning beyond linear separability.\n",
        "\n",
        "Non-linear Activation Functions: Non-linear activation functions are essential in deep neural networks because they allow the network to learn from complex data patterns by introducing non-linearity. Common non-linear activation functions include:\n",
        "\n",
        "Sigmoid (Logistic Function):\n",
        "\n",
        "𝑓\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑥\n",
        "f(x)=\n",
        "1+e\n",
        "−x\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "It squashes the input between 0 and 1. Used commonly in binary classification tasks, but suffers from issues like vanishing gradients, especially in deep networks.\n",
        "\n",
        "Tanh (Hyperbolic Tangent):\n",
        "\n",
        "𝑓\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝑒\n",
        "𝑥\n",
        "−\n",
        "𝑒\n",
        "−\n",
        "𝑥\n",
        "𝑒\n",
        "𝑥\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑥\n",
        "f(x)=\n",
        "e\n",
        "x\n",
        " +e\n",
        "−x\n",
        "\n",
        "e\n",
        "x\n",
        " −e\n",
        "−x\n",
        "\n",
        "​\n",
        "\n",
        "Tanh squashes the input between -1 and 1, offering a zero-centered output. It shares similar issues with sigmoid in terms of vanishing gradients but offers improved convergence over sigmoid.\n",
        "\n",
        "ReLU (Rectified Linear Unit):\n",
        "\n",
        "𝑓\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "max\n",
        "⁡\n",
        "(\n",
        "0\n",
        ",\n",
        "𝑥\n",
        ")\n",
        "f(x)=max(0,x)\n",
        "ReLU introduces sparsity by setting negative inputs to zero, while positive inputs are unchanged. It is computationally efficient and widely used in deep networks, but can suffer from the \"dying ReLU\" problem (neurons can become inactive during training).\n",
        "\n",
        "Leaky ReLU:\n",
        "\n",
        "𝑓\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "max\n",
        "⁡\n",
        "(\n",
        "0.01\n",
        "𝑥\n",
        ",\n",
        "𝑥\n",
        ")\n",
        "f(x)=max(0.01x,x)\n",
        "A variation of ReLU where negative inputs are not set to zero, but are scaled by a small factor (like 0.01). This addresses the dying ReLU problem.\n",
        "\n",
        "Softmax: Used typically in multi-class classification, softmax converts a vector of values into probabilities.\n",
        "\n",
        "Why Non-linear Activation Functions Are Preferred in Hidden Layers:\n",
        "Learning Complex Representations: Non-linear functions allow neural networks to approximate complex relationships in data. Without non-linearity, no matter how many layers are added, the network can only represent linear mappings, severely limiting its capacity to model real-world data.\n",
        "\n",
        "Universal Approximation Theorem: This theorem states that a feedforward neural network with at least one hidden layer and a non-linear activation function can approximate any continuous function to a certain accuracy, given sufficient neurons. Non-linearity is key to this capability.\n",
        "\n",
        "Backpropagation: Non-linear functions are differentiable, which is essential for backpropagation, the learning mechanism of neural networks. The gradients of non-linear activation functions (like ReLU or sigmoid) guide the weight updates during training, allowing the model to improve over time.\n",
        "\n",
        "Linear vs Non-linear Activation Functions:\n",
        "Feature\tLinear Activation Function\tNon-linear Activation Function\n",
        "Formula\n",
        "𝑓\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝑎\n",
        "𝑥\n",
        "+\n",
        "𝑏\n",
        "f(x)=ax+b\tReLU, Sigmoid, Tanh, etc.\n",
        "Learning Capacity\tLimited to linear mappings\tCan model complex, non-linear relationships\n",
        "Layers\tMultiple layers collapse into one linear layer\tMultiple layers build more powerful models\n",
        "Gradient\tConstant (no learning)\tNon-constant, supports backpropagation\n",
        "Real-world Applicability\tPoor, since most data is non-linear\tCan model complex real-world data\n",
        "In summary, non-linear activation functions are essential in hidden layers of neural networks because they introduce the non-linearity needed to model complex functions, learn intricate patterns, and build deeper networks that can solve a wide range of problems across various domains. Without non-linear activation, neural networks would lack the expressive power necessary to tackle real-world challenges."
      ],
      "metadata": {
        "id": "3s8LvrOnnWF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why Non-linear Activation Functions are Preferred in Hidden Layers:\n",
        "\n",
        "Complex Learning: Real-world data is often non-linear. Non-linear activation functions allow neural networks to learn complex data patterns, which linear activation functions cannot achieve.\n",
        "\n",
        "Universal Approximation: According to the universal approximation theorem, a neural network with at least one hidden layer and a non-linear activation function can approximate any continuous function to an arbitrary accuracy, enabling it to solve a wide range of problems.\n",
        "\n",
        "Differentiability: Most non-linear functions are differentiable, which is crucial for gradient-based optimization (such as backpropagation). The gradients help in learning and updating the network's weights during training."
      ],
      "metadata": {
        "id": "q0Ko26NGyNkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Build a simple neural network model\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer with 10 units and ReLU activation\n",
        "model.add(Dense(10, input_dim=8, activation='relu'))\n",
        "\n",
        "# Hidden layer with 20 units and ReLU activation\n",
        "model.add(Dense(20, activation='relu'))\n",
        "\n",
        "# Output layer for binary classification, using sigmoid\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "AxH4ELtanXJM",
        "outputId": "1177eba6-0096-4cab-f4cc-5bb38886708a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │              \u001b[38;5;34m90\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │             \u001b[38;5;34m220\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m21\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">220</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m331\u001b[0m (1.29 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">331</span> (1.29 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m331\u001b[0m (1.29 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">331</span> (1.29 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of Code:\n",
        "Dense layers: Each Dense layer is a fully connected layer in the neural network.\n",
        "ReLU Activation: Used in the hidden layers for introducing non-linearity, enabling the network to learn complex patterns.\n",
        "Sigmoid Activation: Used in the output layer for binary classification (outputs a value between 0 and 1).\n",
        "This model demonstrates the typical use of non-linear activation functions in hidden layers and a sigmoid activation function in the output layer for binary classification.\n",
        "\n",
        "By using non-linear functions like ReLU in hidden layers, the network is capable of learning intricate mappings from input to output, which would not be possible with a purely linear model."
      ],
      "metadata": {
        "id": "8mrSCIRux-Lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.- Describe the Sigmoid activation function. What are its characteristics, and in what type of layers is it\u0015- Describe the Sigmoid activation function. What are its characteristics, and in what type of layers is it\n",
        "commonly used? Explain the Rectified Linear Unit (ReLU) activation function. Discuss its advantages\n",
        "and potential challenges.What is the purpose of the Tanh activation function? How does it differ from\n",
        "the Sigmoid activation function\n",
        "\n",
        "commonly used? Explain the Rectified Linear Unit (ReLU) activation function. Discuss its advantages\n",
        "and potential challenges.What is the purpose of the Tanh activation function? How does it differ from\n",
        "the Sigmoid activation function\n"
      ],
      "metadata": {
        "id": "Mw1-Ij5LylgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Sigmoid Activation Function:\n",
        "Formula:\n",
        "𝑓\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑥\n",
        "f(x)=\n",
        "1+e\n",
        "−x\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "Characteristics:\n",
        "The sigmoid function outputs a value between 0 and 1.\n",
        "It is commonly used in binary classification problems because the output can be interpreted as a probability.\n",
        "It is a smooth, differentiable function, which makes it useful for gradient-based optimization methods.\n",
        "Common Use: It is typically used in the output layer of binary classification problems.\n",
        "Challenges:\n",
        "Vanishing Gradient Problem: In deep networks, the gradient of the sigmoid can become very small, causing the weights to update slowly, which leads to slow learning or no learning.\n",
        "The output is not zero-centered, which can make optimization harder."
      ],
      "metadata": {
        "id": "cGqrBl-By5T0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function implementation\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Example usage\n",
        "x = np.array([-2.0, 0.0, 2.0])\n",
        "sigmoid_output = sigmoid(x)\n",
        "print(\"Sigmoid Output:\", sigmoid_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWC7f4pjy4on",
        "outputId": "5ef9715c-f8f4-4eb5-8ad1-5438ce423834"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid Output: [0.11920292 0.5        0.88079708]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. ReLU (Rectified Linear Unit) Activation Function:\n",
        "Formula:\n",
        "𝑓\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "max\n",
        "⁡\n",
        "(\n",
        "0\n",
        ",\n",
        "𝑥\n",
        ")\n",
        "f(x)=max(0,x)\n",
        "Characteristics:\n",
        "ReLU is the most commonly used activation function in deep learning models.\n",
        "It outputs the input directly if it's positive; otherwise, it outputs zero.\n",
        "It introduces non-linearity into the network while being computationally efficient (simple comparisons).\n",
        "Advantages:\n",
        "Efficient computation: ReLU is very easy to compute.\n",
        "Sparse activation: ReLU encourages sparsity (many neurons output zero), which improves efficiency and makes models more interpretable.\n",
        "Solves the vanishing gradient problem: Since ReLU doesn't saturate for positive inputs, gradients remain strong.\n",
        "Challenges:\n",
        "Dying ReLU problem: Neurons can \"die\" if they always output zero during training, causing them to stop learning.\n",
        "Common Use: Typically used in the hidden layers of a neural network"
      ],
      "metadata": {
        "id": "5hB6ak4CzIp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Example usage\n",
        "x = np.array([-2.0, 0.0, 2.0])\n",
        "relu_output = relu(x)\n",
        "print(\"ReLU Output:\", relu_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kck-hSGKxpBs",
        "outputId": "5b45c3d8-2733-4350-fb70-e1a62bdc51ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ReLU Output: [0. 0. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Tanh (Hyperbolic Tangent) Activation Function:\n",
        "Formula:\n",
        "𝑓\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "2\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "2\n",
        "𝑥\n",
        "−\n",
        "1\n",
        "f(x)=\n",
        "1+e\n",
        "−2x\n",
        "\n",
        "2\n",
        "​\n",
        " −1\n",
        "Characteristics:\n",
        "Tanh outputs values between -1 and 1, which makes it zero-centered, unlike sigmoid.\n",
        "It is similar to the sigmoid function but provides outputs in a wider range.\n",
        "Advantages:\n",
        "Zero-centered output: This property can make optimization easier since gradients won’t oscillate between positive and negative values.\n",
        "Better for hidden layers than sigmoid, especially in deep neural networks.\n",
        "Challenges: Like the sigmoid function, Tanh can also suffer from the vanishing gradient problem when used in deep networks.\n",
        "Common Use: Typically used in the hidden layers of a neural network"
      ],
      "metadata": {
        "id": "nYQxL0J8zVgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "# Example usage\n",
        "x = np.array([-2.0, 0.0, 2.0])\n",
        "tanh_output = tanh(x)\n",
        "print(\"Tanh Output:\", tanh_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfcebXk1zN9v",
        "outputId": "d1a73f76-b2d2-4b77-ed41-a5525ff9b65a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tanh Output: [-0.96402758  0.          0.96402758]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison of Sigmoid vs Tanh:\n",
        "Sigmoid outputs values between 0 and 1, while Tanh outputs between -1 and 1.\n",
        "Tanh is zero-centered, making it easier for the network to optimize as gradients won’t be all in the same direction, unlike the sigmoid.\n",
        "Both can suffer from vanishing gradients, but Tanh usually performs better than sigmoid in practice for hidden layers.\n",
        "Conclusion:\n",
        "Use sigmoid for the output layer of binary classification tasks.\n",
        "Use ReLU for hidden layers in deep networks due to its efficiency and ability to prevent vanishing gradients.\n",
        "Tanh can be used in hidden layers, especially in cases where zero-centered outputs are desirable, although it is less common than ReLU today"
      ],
      "metadata": {
        "id": "Etou39bAzgUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 3. Discuss the significance of activation functions in the hidden layers of a neural network- do the coding"
      ],
      "metadata": {
        "id": "-8l2IW9i3hp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation functions are crucial components in the hidden layers of neural networks, significantly influencing how these networks learn and represent data. Here's an exploration of their significance, along with some coding examples to illustrate how different activation functions can be implemented in Python.\n",
        "\n",
        "Significance of Activation Functions in Hidden Layers\n",
        "Introducing Non-Linearity:\n",
        "\n",
        "Activation functions allow the model to learn complex patterns by introducing non-linearity into the network. This is essential because many real-world problems are inherently non-linear.\n",
        "Without activation functions, regardless of the number of layers, a neural network would behave like a linear transformation, limiting its expressiveness.\n",
        "Facilitating Learning:\n",
        "\n",
        "They help propagate gradients back through the network during training. This is particularly important for deep networks where gradients can vanish or explode.\n",
        "Non-linear activation functions, like ReLU (Rectified Linear Unit), mitigate issues like the vanishing gradient problem, enhancing the training efficiency.\n",
        "Function Approximation:\n",
        "\n",
        "Neural networks aim to approximate complex functions, and activation functions play a key role in this process. They allow the network to fit a variety of data distributions.\n",
        "For instance, a neural network can approximate functions in image processing, speech recognition, and more.\n",
        "Diverse Representations:\n",
        "\n",
        "Different activation functions can be chosen based on the task at hand. This provides flexibility in how features are represented within the network.\n",
        "For example, ReLU is often preferred for hidden layers due to its computational efficiency and effectiveness in handling large datasets."
      ],
      "metadata": {
        "id": "073d-bgX3n9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Example usage\n",
        "logits = np.array([-1.0, 0.0, 1.0])\n",
        "sigmoid_output = sigmoid(logits)\n",
        "print(\"Sigmoid Output:\", sigmoid_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmu_Px1F368z",
        "outputId": "a856ee32-8d54-472b-b1bd-0fa6037bd3dd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid Output: [0.26894142 0.5        0.73105858]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "# Example usage\n",
        "tanh_output = tanh(logits)\n",
        "print(\"Tanh Output:\", tanh_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAWo02Mj4BkK",
        "outputId": "9368547d-78f9-4447-c631-9b920185b655"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tanh Output: [-0.76159416  0.          0.76159416]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Example usage\n",
        "relu_output = relu(logits)\n",
        "print(\"ReLU Output:\", relu_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msGkt78O4BXj",
        "outputId": "75cb846b-10c7-4151-98db-d6cfda907b49"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ReLU Output: [0. 0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def leaky_relu(x, alpha=0.01):\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "# Example usage\n",
        "leaky_relu_output = leaky_relu(logits)\n",
        "print(\"Leaky ReLU Output:\", leaky_relu_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI7pyHzs4BUs",
        "outputId": "9a9d20f7-df46-423c-ce27-350dd98af460"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leaky ReLU Output: [-0.01  0.    1.  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Explain the choice of activation functions for different types of problems (e.g., classification,\n",
        "regression) in the output layer"
      ],
      "metadata": {
        "id": "Jbe-IcCA09Ya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing the right activation function for the output layer of a neural network is crucial, as it directly affects how the model interprets the output and performs for different types of problems. Here’s an explanation of common activation functions used in the output layer, along with Python code examples to illustrate their use.\n",
        "\n",
        "1. Binary Classification\n",
        "For binary classification tasks, where the output is a single label (0 or 1), the sigmoid activation function is typically used.\n",
        "\n",
        "Sigmoid Activation Function\n",
        "Purpose: Outputs a probability value between 0 and 1.\n",
        "Use Case: Suitable for problems like spam detection, where an email is classified as either spam (1) or not spam (0)."
      ],
      "metadata": {
        "id": "xorrS5If1Ey6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Example usage for binary classification\n",
        "logits = np.array([0.0, 1.0, -1.0])  # Example logits\n",
        "probabilities = sigmoid(logits)\n",
        "print(\"Sigmoid Output for Binary Classification:\", probabilities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAjGMIww0-mJ",
        "outputId": "1c7b321d-3d70-48e1-ab5c-052877a6e1d3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid Output for Binary Classification: [0.5        0.73105858 0.26894142]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-class Classification\n",
        "For tasks where the output can belong to one of several classes (more than two), the softmax activation function is used.\n",
        "\n",
        "Softmax Activation Function\n",
        "Purpose: Converts logits into a probability distribution across multiple classes.\n",
        "Use Case: Ideal for tasks like image classification (e.g., classifying images of animals into categories like cat, dog, bird)."
      ],
      "metadata": {
        "id": "ZujJHkhB1SMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
        "    return exp_x / exp_x.sum(axis=0)\n",
        "\n",
        "# Example usage for multi-class classification\n",
        "logits = np.array([2.0, 1.0, 0.1])  # Example logits for three classes\n",
        "probabilities = softmax(logits)\n",
        "print(\"Softmax Output for Multi-class Classification:\", probabilities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxbcNSCv1Lm4",
        "outputId": "5556ebac-46a6-4577-ec33-5eb151615c23"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax Output for Multi-class Classification: [0.65900114 0.24243297 0.09856589]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ". Multi-label Classification\n",
        "In multi-label classification tasks, where each instance can belong to multiple classes simultaneously, the sigmoid function is applied independently to each output node."
      ],
      "metadata": {
        "id": "syGhZDuu1d23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the same sigmoid function as above\n",
        "\n",
        "# Example usage for multi-label classification\n",
        "logits = np.array([[0.5, -0.5], [1.5, 0.0]])  # Example logits for two instances and two classes\n",
        "probabilities = sigmoid(logits)\n",
        "print(\"Sigmoid Output for Multi-label Classification:\", probabilities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjZwritM1W1f",
        "outputId": "ef84827f-ce26-4600-a8ae-a21bb920e279"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid Output for Multi-label Classification: [[0.62245933 0.37754067]\n",
            " [0.81757448 0.5       ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ". Regression Problems\n",
        "For regression tasks, where the output is a continuous value, the linear activation function is commonly used.\n",
        "\n",
        "Linear Activation Function\n",
        "Purpose: No transformation; the output is the same as the input.\n",
        "Use Case: Useful in predicting real-valued outputs, such as house prices."
      ],
      "metadata": {
        "id": "QEzOGRIv1oJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear activation function (identity function)\n",
        "def linear(x):\n",
        "    return x  # No change\n",
        "\n",
        "# Example usage for regression\n",
        "logits = np.array([150000, 200000, 250000])  # Example logits for housing prices\n",
        "predictions = linear(logits)\n",
        "print(\"Linear Output for Regression:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5CRcJk61hU9",
        "outputId": "93652491-7d08-4371-b1cb-cbeb0c9d67c6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Output for Regression: [150000 200000 250000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "summary\n",
        "Binary Classification: Use sigmoid to output a probability.\n",
        "Multi-class Classification: Use softmax to provide a probability distribution across classes.\n",
        "Multi-label Classification: Use sigmoid independently for each label.\n",
        "Regression: Use linear activation to predict continuous values."
      ],
      "metadata": {
        "id": "lbFw-5mL17qC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Experiment with different activation functions (e.g., ReLU, Sigmoid, Tanh) in a simple neural network\n",
        "architecture. Compare their effects on convergence and performance"
      ],
      "metadata": {
        "id": "xiTYBEXf4f0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To experiment with different activation functions like ReLU, Sigmoid, and Tanh in a simple neural network architecture, we can use a popular deep learning framework such as TensorFlow or PyTorch. This experiment will involve training a neural network on a dataset (e.g., the MNIST dataset) and comparing the effects of different activation functions on convergence speed and overall performance.\n",
        "\n",
        "Neural Network Setup\n",
        "Here’s a basic outline of how to set up and run the experiment using PyTorch. The following steps will guide you through building a neural network that uses different activation functions, training it, and evaluating its performance."
      ],
      "metadata": {
        "id": "v2migxcO5vBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n"
      ],
      "metadata": {
        "id": "zW7WSUaD10t1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations to apply to the images\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load training and test datasets\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYhjzLTy58Zo",
        "outputId": "071bfac7-d2e0-4796-977c-56adb0b3a988"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 34.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.19MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 9.78MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 3.13MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, activation_function):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.activation = activation_function\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the input\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)    # Apply the activation function\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "PULaST2I6Gl4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(activation_function):\n",
        "    model = SimpleNN(activation_function)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(5):  # Train for 5 epochs\n",
        "        for images, labels in trainloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "NBU4xkzI6q57"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare different activation functions\n",
        "relu_model = train_model(nn.ReLU())\n",
        "sigmoid_model = train_model(nn.Sigmoid())\n",
        "tanh_model = train_model(nn.Tanh())\n"
      ],
      "metadata": {
        "id": "WXTrX_uc6rzI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "# Evaluate each model\n",
        "relu_accuracy = evaluate_model(relu_model)\n",
        "sigmoid_accuracy = evaluate_model(sigmoid_model)\n",
        "tanh_accuracy = evaluate_model(tanh_model)\n",
        "\n",
        "print(f\"ReLU Accuracy: {relu_accuracy:.2f}\")\n",
        "print(f\"Sigmoid Accuracy: {sigmoid_accuracy:.2f}\")\n",
        "print(f\"Tanh Accuracy: {tanh_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GgSj4O46yrr",
        "outputId": "06c25a8e-e425-4622-9a8a-65507408935e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ReLU Accuracy: 0.97\n",
            "Sigmoid Accuracy: 0.96\n",
            "Tanh Accuracy: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected Results and Discussion\n",
        "ReLU: Generally exhibits faster convergence and better performance on deeper networks due to its non-saturating nature, helping to avoid the vanishing gradient problem.\n",
        "Sigmoid: Often leads to slower convergence, especially in deeper networks due to the saturation of gradients.\n",
        "Tanh: Usually performs better than the sigmoid function because it is zero-centered but can still suffer from the vanishing gradient problem.\n",
        "Conclusion\n",
        "By implementing this simple neural network, you can observe how different activation functions affect convergence speed and model accuracy. Typically, ReLU will outperform Sigmoid and Tanh in most deep learning tasks due to its advantages in training efficiency and gradient propagation.\n",
        "\n",
        "For more in-depth comparisons and further experimentation, you can refer to the following resources:"
      ],
      "metadata": {
        "id": "B74vvWUI7Vc_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-hUrz2er7KIx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}